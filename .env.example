# VK API token — get yours at https://vkhost.github.io
VK_TOKEN=your_vk_token_here

# PulseAudio server address
# Local run (default): unix:/run/user/1000/pulse/native  (or $XDG_RUNTIME_DIR/pulse/native)
# Docker via socket:   unix:/tmp/pulse-native
# Docker via TCP:      tcp:host.docker.internal:4713
PULSE_SERVER=unix:/run/user/1000/pulse/native

# Playback mode: stream | file
MODE=stream

# Temp directory for downloaded tracks (file mode only)
TEMP_DIR=/tmp/music-mcp

# ---------------------------------------------------------------------------
# LLM settings (used by test_tool_loop.py)
# ---------------------------------------------------------------------------

# LLM provider: local | openai | anthropic | openrouter | ollama | custom
#   local      — llama-server on localhost (default, no API key required)
#   openai     — OpenAI API (gpt-4o, gpt-4.1, o3, ...)
#   anthropic  — Anthropic Claude API (claude-opus-4-5, claude-sonnet-4-5, ...)
#   openrouter — OpenRouter proxy (access to many models via one key)
#   ollama     — local Ollama (no API key required)
#   custom     — any OpenAI-compatible endpoint; set LLM_URL to point at it
LLM_PROVIDER=local

# API key for cloud providers (leave blank for local/ollama)
LLM_API_KEY=

# Base URL override (leave blank to use the default for your provider)
#   local      default: http://localhost:8000/v1
#   openai     default: https://api.openai.com/v1
#   anthropic  default: https://api.anthropic.com
#   openrouter default: https://openrouter.ai/api/v1
#   ollama     default: http://localhost:11434/v1
LLM_URL=

# Model name (pick one supported by your provider)
LLM_MODEL=gpt-oss-20b-Q4_K_M.gguf

# mcpo endpoint
MCPO_URL=http://localhost:8001
