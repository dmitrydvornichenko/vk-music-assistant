# VK API token — managed automatically by auth_server.py (auto-refreshed every 6 h)
# You can also set it manually: https://vkhost.github.io
VK_TOKEN=your_vk_token_here

# VK Application ID — used by auth_server.py (hardcoded to vkhost.github.io app, no need to change)
# VK_APP_ID=6287487

# ── auth_server.py settings ─────────────────────────────────────────────────

# HTTP port for the auth server (remote-browser UI + token refresh)
AUTH_PORT=8080

# Path to the Playwright browser session state (cookies, localStorage)
# Saved after first auth; reused for silent token refresh
STATE_FILE=playwright_state.json

# Token refresh interval in seconds (default: 6 hours)
# Set to 0 to disable automatic refresh
TOKEN_REFRESH_INTERVAL=21600

# PulseAudio server address
# Local run (default): unix:/run/user/1000/pulse/native  (or $XDG_RUNTIME_DIR/pulse/native)
# Docker via socket:   unix:/tmp/pulse-native
# Docker via TCP:      tcp:host.docker.internal:4713
PULSE_SERVER=unix:/run/user/1000/pulse/native

# Playback mode: stream | file
MODE=stream

# Temp directory for downloaded tracks (file mode only)
TEMP_DIR=/tmp/music-mcp

# ---------------------------------------------------------------------------
# LLM settings (used by cli.py)
# ---------------------------------------------------------------------------

# LLM provider: local | openai | anthropic | openrouter | ollama | custom
#   local      — llama-server on localhost (default, no API key required)
#   openai     — OpenAI API (gpt-4o, gpt-4.1, o3, ...)
#   anthropic  — Anthropic Claude API (claude-opus-4-5, claude-sonnet-4-5, ...)
#   openrouter — OpenRouter proxy (access to many models via one key)
#   ollama     — local Ollama (no API key required)
#   custom     — any OpenAI-compatible endpoint; set LLM_URL to point at it
LLM_PROVIDER=local

# API key for cloud providers (leave blank for local/ollama)
LLM_API_KEY=

# Base URL override (leave blank to use the default for your provider)
#   local      default: http://localhost:8000/v1
#   openai     default: https://api.openai.com/v1
#   anthropic  default: https://api.anthropic.com
#   openrouter default: https://openrouter.ai/api/v1
#   ollama     default: http://localhost:11434/v1
LLM_URL=

# Model name (pick one supported by your provider)
LLM_MODEL=gpt-oss-20b-Q4_K_M.gguf

# mcpo endpoint
MCPO_URL=http://localhost:8001

# ---------------------------------------------------------------------------
# Голосовой ассистент (controller): STT → LLM → TTS
# ---------------------------------------------------------------------------
# URL Ollama на хосте (controller подключается к нему для LLM)
# Запусти Ollama локально и укажи: http://host.docker.internal:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434
